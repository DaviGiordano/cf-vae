{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.datasets import fetch_openml\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device and hyperparameters\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "hidden_dim = 512\n",
    "latent_dim = 10  # Adjust based on desired complexity\n",
    "num_epochs = 10\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_adult_data(filepath):\n",
    "    \"\"\"\n",
    "    Loads the Adult Census dataset from a CSV file.\n",
    "\n",
    "    Args:\n",
    "        filepath (str): Path to the CSV file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Loaded dataset.\n",
    "    \"\"\"\n",
    "    adult_data = pd.read_csv(\n",
    "        filepath,\n",
    "        na_values='?',\n",
    "    )\n",
    "    # Rename 'class' column to 'income'\n",
    "    adult_data.rename(columns={'class': 'income'}, inplace=True)\n",
    "    return adult_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(adult_data):\n",
    "    \"\"\"\n",
    "    Splits the data into training and test sets.\n",
    "\n",
    "    Args:\n",
    "        adult_data (pd.DataFrame): The raw dataset.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame, pd.DataFrame, pd.Series, pd.Series: Training and test features and labels.\n",
    "    \"\"\"\n",
    "    # Drop rows with missing values\n",
    "    adult_data.dropna(inplace=True)\n",
    "\n",
    "    # Encode 'income' column using LabelEncoder\n",
    "    label_encoder_income = LabelEncoder()\n",
    "    adult_data['income_encoded'] = label_encoder_income.fit_transform(adult_data['income'])\n",
    "    num_classes = len(label_encoder_income.classes_)\n",
    "\n",
    "    # Features and labels\n",
    "    X = adult_data.drop(columns=['income', 'income_encoded'])  # Drop 'income' from features\n",
    "    y = adult_data['income_encoded']  # Use encoded 'income' as labels\n",
    "\n",
    "    # Split into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y,\n",
    "        test_size=0.2,\n",
    "        stratify=y,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, num_classes, label_encoder_income\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_preprocessing_pipeline(X_train):\n",
    "    \"\"\"\n",
    "    Creates preprocessing pipelines for numerical and categorical data.\n",
    "\n",
    "    Args:\n",
    "        X_train (pd.DataFrame): Training features.\n",
    "\n",
    "    Returns:\n",
    "        ColumnTransformer: Preprocessing pipeline.\n",
    "    \"\"\"\n",
    "    # Identify numerical and categorical columns\n",
    "    numerical_columns = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    categorical_columns = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "    # Remove 'fnlwgt' if desired (it's a weighting factor that may not be useful)\n",
    "    if 'fnlwgt' in numerical_columns:\n",
    "        numerical_columns.remove('fnlwgt')\n",
    "\n",
    "    # Define numerical and categorical pipelines\n",
    "    numerical_pipeline = Pipeline(steps=[\n",
    "        ('scaler', MinMaxScaler())\n",
    "    ])\n",
    "\n",
    "    categorical_pipeline = Pipeline(steps=[\n",
    "        ('onehot', OneHotEncoder())\n",
    "    ])\n",
    "\n",
    "    # Combine pipelines using ColumnTransformer\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numerical_pipeline, numerical_columns),\n",
    "            ('cat', categorical_pipeline, categorical_columns)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return preprocessor, numerical_columns, categorical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdultDataset(Dataset):\n",
    "    def __init__(self, features, income_labels):\n",
    "        self.features = torch.tensor(features, dtype=torch.float32)\n",
    "        self.income_labels = torch.tensor(income_labels.values, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.income_labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class VAEOutput:\n",
    "    \"\"\"\n",
    "    Dataclass for VAE output.\n",
    "    \"\"\"\n",
    "    z_sample: torch.Tensor\n",
    "    x_recon: torch.Tensor\n",
    "    loss: torch.Tensor\n",
    "    loss_recon: torch.Tensor\n",
    "    loss_kl: torch.Tensor\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    \"\"\"\n",
    "    Variational Autoencoder (VAE) class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim, num_classes):\n",
    "        super(VAE, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim + num_classes, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, hidden_dim // 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 4, 2 * latent_dim),  # Mean and log variance\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim + num_classes, hidden_dim // 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 4, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def encode(self, x, y, eps: float = 1e-8):\n",
    "        y_onehot = F.one_hot(y, num_classes=self.num_classes).float()\n",
    "        x = torch.cat([x, y_onehot], dim=-1)\n",
    "        h = self.encoder(x)\n",
    "        mu, logvar = torch.chunk(h, 2, dim=-1)\n",
    "        std = torch.exp(0.5 * logvar) + eps\n",
    "        return mu, std\n",
    "\n",
    "    def reparameterize(self, mu, std):\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z, y):\n",
    "        y_onehot = F.one_hot(y, num_classes=self.num_classes).float()\n",
    "        z = torch.cat([z, y_onehot], dim=-1)\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x, y, compute_loss: bool = True):\n",
    "        mu, std = self.encode(x, y)\n",
    "        z = self.reparameterize(mu, std)\n",
    "        recon_x = self.decode(z, y)\n",
    "\n",
    "        if not compute_loss:\n",
    "            return VAEOutput(\n",
    "                z_sample=z,\n",
    "                x_recon=recon_x,\n",
    "                loss=None,\n",
    "                loss_recon=None,\n",
    "                loss_kl=None,\n",
    "            )\n",
    "\n",
    "        # Reconstruction loss\n",
    "        loss_recon = F.mse_loss(recon_x, x, reduction='mean')\n",
    "\n",
    "        # KL divergence\n",
    "        loss_kl = -0.5 * torch.mean(1 + torch.log(std.pow(2)) - mu.pow(2) - std.pow(2))\n",
    "\n",
    "        loss = loss_recon + loss_kl\n",
    "\n",
    "        return VAEOutput(\n",
    "            z_sample=z,\n",
    "            x_recon=recon_x,\n",
    "            loss=loss,\n",
    "            loss_recon=loss_recon,\n",
    "            loss_kl=loss_kl,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, epoch, device, writer):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(tqdm(dataloader, desc=f\"Training Epoch {epoch+1}\")):\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data, target)\n",
    "        loss = output.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Log training loss every log_interval batches\n",
    "        log_interval = 100\n",
    "        if batch_idx % log_interval == 0:\n",
    "            step = epoch * len(dataloader) + batch_idx\n",
    "            writer.add_scalar('Loss/Train', loss.item(), step)\n",
    "            writer.add_scalar('Loss/Train_Reconstruction', output.loss_recon.item(), step)\n",
    "            writer.add_scalar('Loss/Train_KL', output.loss_kl.item(), step)\n",
    "\n",
    "    average_loss = train_loss / len(dataloader)\n",
    "    print(f\"====> Epoch: {epoch+1} Average training loss: {average_loss:.4f}\")\n",
    "    writer.add_scalar('Loss/Train_Epoch', average_loss, epoch + 1)\n",
    "\n",
    "def test(model, dataloader, epoch, device, writer):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    loss_recon_total = 0\n",
    "    loss_kl_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in tqdm(dataloader, desc='Testing'):\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            output = model(data, target)\n",
    "            test_loss += output.loss.item()\n",
    "            loss_recon_total += output.loss_recon.item()\n",
    "            loss_kl_total += output.loss_kl.item()\n",
    "\n",
    "    average_loss = test_loss / len(dataloader)\n",
    "    average_loss_recon = loss_recon_total / len(dataloader)\n",
    "    average_loss_kl = loss_kl_total / len(dataloader)\n",
    "\n",
    "    print(f\"====> Test set loss: {average_loss:.4f}\")\n",
    "    writer.add_scalar('Loss/Test', average_loss, epoch + 1)\n",
    "    writer.add_scalar('Loss/Test_Reconstruction', average_loss_recon, epoch + 1)\n",
    "    writer.add_scalar('Loss/Test_KL', average_loss_kl, epoch + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_samples(model, num_samples, desired_income, device, preprocessor, numerical_columns, categorical_columns):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(num_samples, model.encoder[-1].out_features // 2).to(device)\n",
    "        y = torch.full((num_samples,), desired_income, dtype=torch.long, device=device)\n",
    "        samples = model.decode(z, y)\n",
    "    samples_np = samples.cpu().numpy()\n",
    "\n",
    "    # Inverse transform the data\n",
    "    # Get the number of numerical features\n",
    "    num_numerical = len(numerical_columns)\n",
    "    num_categorical = samples_np.shape[1] - num_numerical\n",
    "\n",
    "    # Inverse transform numerical data\n",
    "    numerical_data = samples_np[:, :num_numerical]\n",
    "    numerical_data_inv = preprocessor.named_transformers_['num'].inverse_transform(numerical_data)\n",
    "\n",
    "    # Inverse transform categorical data\n",
    "    categorical_data = samples_np[:, num_numerical:]\n",
    "    categorical_data_inv = preprocessor.named_transformers_['cat'].inverse_transform(categorical_data)\n",
    "\n",
    "    # Combine numerical and categorical data\n",
    "    samples_df_num = pd.DataFrame(numerical_data_inv, columns=numerical_columns)\n",
    "    samples_df_cat = pd.DataFrame(categorical_data_inv, columns=categorical_columns)\n",
    "    final_samples_df = pd.concat([samples_df_num, samples_df_cat], axis=1)\n",
    "\n",
    "    return final_samples_df\n",
    "\n",
    "def log_generated_samples(model, device, writer, epoch, preprocessor, numerical_columns, categorical_columns, label_encoder_income):\n",
    "    num_samples = 5\n",
    "    desired_income = 1  # '>50K'\n",
    "\n",
    "    final_samples_df = generate_samples(\n",
    "        model, num_samples, desired_income, device, preprocessor, numerical_columns, categorical_columns\n",
    "    )\n",
    "\n",
    "    # Log the generated samples as text to TensorBoard\n",
    "    for i in range(num_samples):\n",
    "        sample = final_samples_df.iloc[i]\n",
    "        sample_text = sample.to_string()\n",
    "        writer.add_text(f'Generated Samples/Epoch_{epoch+1}_Sample_{i+1}', sample_text, epoch + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Check if 'adult.csv' exists\n",
    "if not os.path.exists('adult.csv'):\n",
    "    # Download the 'adult' census dataset\n",
    "    adult = fetch_openml(name='adult', version=2, as_frame=True)\n",
    "    df = adult.frame\n",
    "    df.to_csv('adult.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "adult_data = load_adult_data('adult.csv')\n",
    "\n",
    "# Split data\n",
    "X_train_raw, X_test_raw, y_train, y_test, num_income_classes, label_encoder_income = split_data(adult_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create preprocessing pipeline\n",
    "preprocessor, numerical_columns, categorical_columns = create_preprocessing_pipeline(X_train_raw)\n",
    "\n",
    "# Fit the preprocessor on training data\n",
    "preprocessor.fit(X_train_raw)\n",
    "\n",
    "# Transform the data\n",
    "X_train = preprocessor.transform(X_train_raw).todense()\n",
    "X_test = preprocessor.transform(X_test_raw).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_dataset = AdultDataset(X_train, y_train)\n",
    "test_dataset = AdultDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model and optimizer\n",
    "model = VAE(input_dim=X_train.shape[1], hidden_dim=hidden_dim, latent_dim=latent_dim, num_classes=num_income_classes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "writer = SummaryWriter(log_dir='runs/conditional_vae_adult')\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    train(model, train_loader, optimizer, epoch, device, writer)\n",
    "    test(model, test_loader, epoch, device, writer)\n",
    "    log_generated_samples(\n",
    "        model, device, writer, epoch, preprocessor, numerical_columns, categorical_columns, label_encoder_income\n",
    "    )\n",
    "    \n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate samples conditioned on a specific income class\n",
    "# For 'income', the classes are encoded as 0: '<=50K', 1: '>50K'\n",
    "desired_income = 0  # Adjust based on your encoding (1 corresponds to '>50K')\n",
    "num_samples = 5\n",
    "\n",
    "final_samples_df = generate_samples(\n",
    "    model, num_samples, desired_income, device, preprocessor, numerical_columns, categorical_columns\n",
    ")\n",
    "\n",
    "# Print the generated samples\n",
    "print(\"\\nGenerated Samples:\")\n",
    "print(final_samples_df.head())\n",
    "\n",
    "# Decode the income label\n",
    "income_class = label_encoder_income.inverse_transform([desired_income])[0]\n",
    "print(f\"\\nSamples conditioned on income: {income_class}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate samples conditioned on a specific income class\n",
    "# For 'income', the classes are encoded as 0: '<=50K', 1: '>50K'\n",
    "desired_income = 1  # Adjust based on your encoding (1 corresponds to '>50K')\n",
    "num_samples = 5\n",
    "\n",
    "final_samples_df = generate_samples(\n",
    "    model, num_samples, desired_income, device, preprocessor, numerical_columns, categorical_columns\n",
    ")\n",
    "\n",
    "# Print the generated samples\n",
    "print(\"\\nGenerated Samples:\")\n",
    "print(final_samples_df.head())\n",
    "\n",
    "# Decode the income label\n",
    "income_class = label_encoder_income.inverse_transform([desired_income])[0]\n",
    "print(f\"\\nSamples conditioned on income: {income_class}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vae_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
