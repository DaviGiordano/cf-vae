{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.datasets import fetch_openml\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device and hyperparameters\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "hidden_dim = 512\n",
    "latent_dim = 10  # Adjust based on desired complexity\n",
    "num_epochs = 10\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_adult_data(filepath):\n",
    "    \"\"\"\n",
    "    Loads the Adult Census dataset from a CSV file.\n",
    "\n",
    "    Args:\n",
    "        filepath (str): Path to the CSV file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Loaded dataset.\n",
    "    \"\"\"\n",
    "    adult_data = pd.read_csv(\n",
    "        filepath,\n",
    "        na_values='?',\n",
    "    )\n",
    "    # Rename 'class' column to 'income'\n",
    "    adult_data.rename(columns={'class': 'income'}, inplace=True)\n",
    "    return adult_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(adult_data):\n",
    "    \"\"\"\n",
    "    Splits the data into training and test sets.\n",
    "\n",
    "    Args:\n",
    "        adult_data (pd.DataFrame): The raw dataset.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame, pd.DataFrame, pd.Series, pd.Series: Training and test features and labels.\n",
    "    \"\"\"\n",
    "    # Drop rows with missing values\n",
    "    adult_data.dropna(inplace=True)\n",
    "\n",
    "    # Encode 'income' column using LabelEncoder\n",
    "    label_encoder_income = LabelEncoder()\n",
    "    adult_data['income_encoded'] = label_encoder_income.fit_transform(adult_data['income'])\n",
    "    num_classes = len(label_encoder_income.classes_)\n",
    "\n",
    "    # Features and labels\n",
    "    X = adult_data.drop(columns=['income', 'income_encoded'])  # Drop 'income' from features\n",
    "    y = adult_data['income_encoded']  # Use encoded 'income' as labels\n",
    "\n",
    "    # Split into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y,\n",
    "        test_size=0.2,\n",
    "        stratify=y,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, num_classes, label_encoder_income\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_preprocessing_pipeline(X_train):\n",
    "    \"\"\"\n",
    "    Creates preprocessing pipelines for numerical and categorical data.\n",
    "\n",
    "    Args:\n",
    "        X_train (pd.DataFrame): Training features.\n",
    "\n",
    "    Returns:\n",
    "        ColumnTransformer: Preprocessing pipeline.\n",
    "    \"\"\"\n",
    "    # Identify numerical and categorical columns\n",
    "    numerical_columns = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    categorical_columns = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "    # Remove 'fnlwgt' if desired (it's a weighting factor that may not be useful)\n",
    "    if 'fnlwgt' in numerical_columns:\n",
    "        numerical_columns.remove('fnlwgt')\n",
    "\n",
    "    # Define numerical and categorical pipelines\n",
    "    numerical_pipeline = Pipeline(steps=[\n",
    "        ('scaler', MinMaxScaler())\n",
    "    ])\n",
    "\n",
    "    categorical_pipeline = Pipeline(steps=[\n",
    "        ('onehot', OneHotEncoder())\n",
    "    ])\n",
    "\n",
    "    # Combine pipelines using ColumnTransformer\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numerical_pipeline, numerical_columns),\n",
    "            ('cat', categorical_pipeline, categorical_columns)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return preprocessor, numerical_columns, categorical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdultDataset(Dataset):\n",
    "    def __init__(self, features, income_labels):\n",
    "        self.features = torch.tensor(features, dtype=torch.float32)\n",
    "        self.income_labels = torch.tensor(income_labels.values, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.income_labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class VAEOutput:\n",
    "    \"\"\"\n",
    "    Dataclass for VAE output.\n",
    "    \"\"\"\n",
    "    z_sample: torch.Tensor\n",
    "    x_recon: torch.Tensor\n",
    "    loss: torch.Tensor\n",
    "    loss_recon: torch.Tensor\n",
    "    loss_kl: torch.Tensor\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    \"\"\"\n",
    "    Variational Autoencoder (VAE) class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim, num_classes):\n",
    "        super(VAE, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim + num_classes, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, hidden_dim // 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 4, 2 * latent_dim),  # Mean and log variance\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim + num_classes, hidden_dim // 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 4, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def encode(self, x, y, eps: float = 1e-8):\n",
    "        y_onehot = F.one_hot(y, num_classes=self.num_classes).float()\n",
    "        x = torch.cat([x, y_onehot], dim=-1)\n",
    "        h = self.encoder(x)\n",
    "        mu, logvar = torch.chunk(h, 2, dim=-1)\n",
    "        std = torch.exp(0.5 * logvar) + eps\n",
    "        return mu, std\n",
    "\n",
    "    def reparameterize(self, mu, std):\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z, y):\n",
    "        y_onehot = F.one_hot(y, num_classes=self.num_classes).float()\n",
    "        z = torch.cat([z, y_onehot], dim=-1)\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x, y, compute_loss: bool = True):\n",
    "        mu, std = self.encode(x, y)\n",
    "        z = self.reparameterize(mu, std)\n",
    "        recon_x = self.decode(z, y)\n",
    "\n",
    "        if not compute_loss:\n",
    "            return VAEOutput(\n",
    "                z_sample=z,\n",
    "                x_recon=recon_x,\n",
    "                loss=None,\n",
    "                loss_recon=None,\n",
    "                loss_kl=None,\n",
    "            )\n",
    "\n",
    "        # Reconstruction loss\n",
    "        loss_recon = F.mse_loss(recon_x, x, reduction='mean')\n",
    "\n",
    "        # KL divergence\n",
    "        loss_kl = -0.5 * torch.mean(1 + torch.log(std.pow(2)) - mu.pow(2) - std.pow(2))\n",
    "\n",
    "        loss = loss_recon + loss_kl\n",
    "\n",
    "        return VAEOutput(\n",
    "            z_sample=z,\n",
    "            x_recon=recon_x,\n",
    "            loss=loss,\n",
    "            loss_recon=loss_recon,\n",
    "            loss_kl=loss_kl,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, epoch, device, writer):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(tqdm(dataloader, desc=f\"Training Epoch {epoch+1}\")):\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data, target)\n",
    "        loss = output.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Log training loss every log_interval batches\n",
    "        log_interval = 100\n",
    "        if batch_idx % log_interval == 0:\n",
    "            step = epoch * len(dataloader) + batch_idx\n",
    "            writer.add_scalar('Loss/Train', loss.item(), step)\n",
    "            writer.add_scalar('Loss/Train_Reconstruction', output.loss_recon.item(), step)\n",
    "            writer.add_scalar('Loss/Train_KL', output.loss_kl.item(), step)\n",
    "\n",
    "    average_loss = train_loss / len(dataloader)\n",
    "    print(f\"====> Epoch: {epoch+1} Average training loss: {average_loss:.4f}\")\n",
    "    writer.add_scalar('Loss/Train_Epoch', average_loss, epoch + 1)\n",
    "\n",
    "def test(model, dataloader, epoch, device, writer):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    loss_recon_total = 0\n",
    "    loss_kl_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in tqdm(dataloader, desc='Testing'):\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            output = model(data, target)\n",
    "            test_loss += output.loss.item()\n",
    "            loss_recon_total += output.loss_recon.item()\n",
    "            loss_kl_total += output.loss_kl.item()\n",
    "\n",
    "    average_loss = test_loss / len(dataloader)\n",
    "    average_loss_recon = loss_recon_total / len(dataloader)\n",
    "    average_loss_kl = loss_kl_total / len(dataloader)\n",
    "\n",
    "    print(f\"====> Test set loss: {average_loss:.4f}\")\n",
    "    writer.add_scalar('Loss/Test', average_loss, epoch + 1)\n",
    "    writer.add_scalar('Loss/Test_Reconstruction', average_loss_recon, epoch + 1)\n",
    "    writer.add_scalar('Loss/Test_KL', average_loss_kl, epoch + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_samples(model, num_samples, desired_income, device, preprocessor, numerical_columns, categorical_columns):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(num_samples, model.encoder[-1].out_features // 2).to(device)\n",
    "        y = torch.full((num_samples,), desired_income, dtype=torch.long, device=device)\n",
    "        samples = model.decode(z, y)\n",
    "    samples_np = samples.cpu().numpy()\n",
    "\n",
    "    # Inverse transform the data\n",
    "    # Get the number of numerical features\n",
    "    num_numerical = len(numerical_columns)\n",
    "    num_categorical = samples_np.shape[1] - num_numerical\n",
    "\n",
    "    # Inverse transform numerical data\n",
    "    numerical_data = samples_np[:, :num_numerical]\n",
    "    numerical_data_inv = preprocessor.named_transformers_['num'].inverse_transform(numerical_data)\n",
    "\n",
    "    # Inverse transform categorical data\n",
    "    categorical_data = samples_np[:, num_numerical:]\n",
    "    categorical_data_inv = preprocessor.named_transformers_['cat'].inverse_transform(categorical_data)\n",
    "\n",
    "    # Combine numerical and categorical data\n",
    "    samples_df_num = pd.DataFrame(numerical_data_inv, columns=numerical_columns)\n",
    "    samples_df_cat = pd.DataFrame(categorical_data_inv, columns=categorical_columns)\n",
    "    final_samples_df = pd.concat([samples_df_num, samples_df_cat], axis=1)\n",
    "\n",
    "    return final_samples_df\n",
    "\n",
    "def log_generated_samples(model, device, writer, epoch, preprocessor, numerical_columns, categorical_columns, label_encoder_income):\n",
    "    num_samples = 5\n",
    "    desired_income = 1  # '>50K'\n",
    "\n",
    "    final_samples_df = generate_samples(\n",
    "        model, num_samples, desired_income, device, preprocessor, numerical_columns, categorical_columns\n",
    "    )\n",
    "\n",
    "    # Log the generated samples as text to TensorBoard\n",
    "    for i in range(num_samples):\n",
    "        sample = final_samples_df.iloc[i]\n",
    "        sample_text = sample.to_string()\n",
    "        writer.add_text(f'Generated Samples/Epoch_{epoch+1}_Sample_{i+1}', sample_text, epoch + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Check if 'adult.csv' exists\n",
    "if not os.path.exists('adult.csv'):\n",
    "    # Download the 'adult' census dataset\n",
    "    adult = fetch_openml(name='adult', version=2, as_frame=True)\n",
    "    df = adult.frame\n",
    "    df.to_csv('adult.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "adult_data = load_adult_data('adult.csv')\n",
    "\n",
    "# Split data\n",
    "X_train_raw, X_test_raw, y_train, y_test, num_income_classes, label_encoder_income = split_data(adult_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create preprocessing pipeline\n",
    "preprocessor, numerical_columns, categorical_columns = create_preprocessing_pipeline(X_train_raw)\n",
    "\n",
    "# Fit the preprocessor on training data\n",
    "preprocessor.fit(X_train_raw)\n",
    "\n",
    "# Transform the data\n",
    "X_train = preprocessor.transform(X_train_raw).todense()\n",
    "X_test = preprocessor.transform(X_test_raw).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_dataset = AdultDataset(X_train, y_train)\n",
    "test_dataset = AdultDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9e0717bd9f94bafae86851f32f36b08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 1:   0%|          | 0/283 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 1 Average training loss: 0.0487\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "444ccafd7eec4bc19f567a359646e2c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing:   0%|          | 0/71 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Test set loss: 0.0420\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecfee40870264e85a8eb8ee038e997af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 2:   0%|          | 0/283 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 2 Average training loss: 0.0419\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a006f310070b4e1da30c2918fe488c24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing:   0%|          | 0/71 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Test set loss: 0.0417\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a54db689dbc49c2b84c8d12604e866d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 3:   0%|          | 0/283 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 3 Average training loss: 0.0418\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d50de6054b746089f3ad5f580621b11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing:   0%|          | 0/71 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Test set loss: 0.0416\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0198309f0cc243ae9a42066d6aa7c2bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 4:   0%|          | 0/283 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 4 Average training loss: 0.0418\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "732dda5614064443a12365c918b77119",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing:   0%|          | 0/71 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Test set loss: 0.0417\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c1e688a8dc14836828e75e8e3106a7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 5:   0%|          | 0/283 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 5 Average training loss: 0.0418\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7514887a36314707850831dc64d6fe5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing:   0%|          | 0/71 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Test set loss: 0.0417\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab686f95cefc4c50bbcfaebc479c7eb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 6:   0%|          | 0/283 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 6 Average training loss: 0.0418\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9af2c14d57fd42a091e4b565ede6b53a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing:   0%|          | 0/71 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Test set loss: 0.0417\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2570da697764dbba6c6761f7da10310",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 7:   0%|          | 0/283 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 7 Average training loss: 0.0418\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "225c171440cf488b9ba06b0dcfc2f8d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing:   0%|          | 0/71 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Test set loss: 0.0418\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea17ec5fe1c644a7a8089506e4f717b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 8:   0%|          | 0/283 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 8 Average training loss: 0.0418\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f14475b0a654eee93c277d8ad913fff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing:   0%|          | 0/71 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Test set loss: 0.0416\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7324e178f8084542b2611e0d92d8ed74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 9:   0%|          | 0/283 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 9 Average training loss: 0.0418\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55c3287db963477c8927f960dd15a373",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing:   0%|          | 0/71 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Test set loss: 0.0417\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9f6dd7d7d87453e9737429b8683f4db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 10:   0%|          | 0/283 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 10 Average training loss: 0.0418\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e0ed7f2abd84674aef32fb86e79286a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing:   0%|          | 0/71 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Test set loss: 0.0417\n"
     ]
    }
   ],
   "source": [
    "# Initialize model and optimizer\n",
    "model = VAE(input_dim=X_train.shape[1], hidden_dim=hidden_dim, latent_dim=latent_dim, num_classes=num_income_classes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "writer = SummaryWriter(log_dir='runs/conditional_vae_adult')\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    train(model, train_loader, optimizer, epoch, device, writer)\n",
    "    test(model, test_loader, epoch, device, writer)\n",
    "    log_generated_samples(\n",
    "        model, device, writer, epoch, preprocessor, numerical_columns, categorical_columns, label_encoder_income\n",
    "    )\n",
    "    \n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Samples:\n",
      "         age  education-num  capital-gain  capital-loss  hours-per-week  \\\n",
      "0  37.818928       9.678803    928.586304     76.391281       38.970665   \n",
      "1  37.906044       9.671959    958.353760     78.512070       39.039623   \n",
      "2  37.697903       9.681820    897.575623     74.269279       38.879658   \n",
      "3  37.837624       9.675223    938.164673     77.087257       38.984947   \n",
      "4  38.060295       9.657116   1015.300476     82.471115       39.175804   \n",
      "\n",
      "  workclass education marital-status     occupation relationship   race   sex  \\\n",
      "0   Private   HS-grad  Never-married  Other-service      Husband  White  Male   \n",
      "1   Private   HS-grad  Never-married  Other-service      Husband  White  Male   \n",
      "2   Private   HS-grad  Never-married  Other-service      Husband  White  Male   \n",
      "3   Private   HS-grad  Never-married  Other-service      Husband  White  Male   \n",
      "4   Private   HS-grad  Never-married  Other-service      Husband  White  Male   \n",
      "\n",
      "  native-country  \n",
      "0  United-States  \n",
      "1  United-States  \n",
      "2  United-States  \n",
      "3  United-States  \n",
      "4  United-States  \n",
      "\n",
      "Samples conditioned on income: <=50K\n"
     ]
    }
   ],
   "source": [
    "# Generate samples conditioned on a specific income class\n",
    "# For 'income', the classes are encoded as 0: '<=50K', 1: '>50K'\n",
    "desired_income = 0  # Adjust based on your encoding (1 corresponds to '>50K')\n",
    "num_samples = 5\n",
    "\n",
    "final_samples_df = generate_samples(\n",
    "    model, num_samples, desired_income, device, preprocessor, numerical_columns, categorical_columns\n",
    ")\n",
    "\n",
    "# Print the generated samples\n",
    "print(\"\\nGenerated Samples:\")\n",
    "print(final_samples_df.head())\n",
    "\n",
    "# Decode the income label\n",
    "income_class = label_encoder_income.inverse_transform([desired_income])[0]\n",
    "print(f\"\\nSamples conditioned on income: {income_class}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Samples:\n",
      "         age  education-num  capital-gain  capital-loss  hours-per-week  \\\n",
      "0  43.407150      11.547520   2860.858643    171.151993       43.669777   \n",
      "1  43.009159      11.521250   2576.706299    157.152084       43.380733   \n",
      "2  43.569908      11.465553   3096.794189    184.053070       43.790455   \n",
      "3  43.159351      11.500388   2719.616211    164.675110       43.503033   \n",
      "4  43.051792      11.539227   2583.427490    157.238907       43.412880   \n",
      "\n",
      "  workclass  education      marital-status       occupation relationship  \\\n",
      "0   Private  Bachelors  Married-civ-spouse  Exec-managerial      Husband   \n",
      "1   Private  Bachelors  Married-civ-spouse  Exec-managerial      Husband   \n",
      "2   Private  Bachelors  Married-civ-spouse  Exec-managerial      Husband   \n",
      "3   Private  Bachelors  Married-civ-spouse  Exec-managerial      Husband   \n",
      "4   Private  Bachelors  Married-civ-spouse  Exec-managerial      Husband   \n",
      "\n",
      "    race   sex native-country  \n",
      "0  White  Male  United-States  \n",
      "1  White  Male  United-States  \n",
      "2  White  Male  United-States  \n",
      "3  White  Male  United-States  \n",
      "4  White  Male  United-States  \n",
      "\n",
      "Samples conditioned on income: >50K\n"
     ]
    }
   ],
   "source": [
    "# Generate samples conditioned on a specific income class\n",
    "# For 'income', the classes are encoded as 0: '<=50K', 1: '>50K'\n",
    "desired_income = 1  # Adjust based on your encoding (1 corresponds to '>50K')\n",
    "num_samples = 5\n",
    "\n",
    "final_samples_df = generate_samples(\n",
    "    model, num_samples, desired_income, device, preprocessor, numerical_columns, categorical_columns\n",
    ")\n",
    "\n",
    "# Print the generated samples\n",
    "print(\"\\nGenerated Samples:\")\n",
    "print(final_samples_df.head())\n",
    "\n",
    "# Decode the income label\n",
    "income_class = label_encoder_income.inverse_transform([desired_income])[0]\n",
    "print(f\"\\nSamples conditioned on income: {income_class}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vae_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
